---
title: "Monitor the Twitter Feed"
author: "The Hackathon Team"
date: "30 November 2016"
output:
 html_document:
   self_contained: no
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(fig.align = "center", message = FALSE)
options(width=100)
```

This tutorial illustrates how one extracts data from twitter
using `R` in order to quickly establish a tweet based monitoring
system for critical event monitoring.
We will use the `rtweet` package to harvest the data using the
[twitter API](https://dev.twitter.com/overview/api) and subsequently
monitor them with the `surveillance` package.

## Setting up the twitter API

The [README.md](https://github.com/mkearney/rtweet) of the `rtweet`
package provides helpful information on how to create a twitter app to
automatically search tweets using the twitter AP

```{r, message=FALSE}
library(rtweet)
library(ggplot2)
library(dplyr)
```

```{r,eval=FALSE}
# A file containing the information of your twitter app. To protect the
# information stored here, this is kept outside the public git repository.
#
# The contents of the file are simply:
# twitter_token <- create_token(app = "surveillance_trends", # whatever you named app
#   consumer_key = "2zIXXX4L6USa4UfXXXXXXXXXX",
#   consumer_secret = "oXXXXXXXSwwXXXXXXXXXXXXXXsXXXXXXXmXXXXXXXX")

source("~/Sandbox/Twitter-Trends/auth-trends.R",encoding="UTF8")
```

## Performing Queries

Perform the query, here we shall search for tweets containing the
hashtag `#ESCAIDE2016`. The result is a list of individual tweets --
this is almost similar to the well known concept of a linelist:

```{r}
the_query <- "#ESCAIDE2016 OR #ESCAIDE OR @ESCAIDE"
```

```{r,eval=FALSE}
tweets <- search_tweets(the_query, n = 15000, type="recent", token = twitter_token)
head(tweets)
nrow(tweets)
```

````{r,echo=FALSE,eval=FALSE}
saveRDS(object=tweets,file="data/tweets.rds")
```

```{r,echo=FALSE}
tweets <- readRDS(file="data/tweets.rds")
```

A total of `r nrow(tweets)` tweets were collected.

### Descriptive Analysis

Who's tweeting? The tweets originate from a total of
`r n_distinct(tweets$screen_name)` users. The top 25 users (by their
number of tweets) are:

```{r,message=FALSE}

top_tweeters <- tweets %>% group_by(screen_name) %>%
  summarise(nTweets=n()) %>%
  arrange(desc(nTweets)) %>%
  top_n(n=25)

ggplot( top_tweeters, aes(x=screen_name,weight=nTweets)) + geom_bar() + coord_flip() + xlab("Number of tweets")
```

Distribution of the different hashtags used in the tweets:

```{r}
df <- data.frame(hashtag=unlist((tweets)$hashtags)) %>%
  mutate(hashtag = stringr::str_trim(hashtag), hashtag = tolower(hashtag)) %>%
  filter(!hashtag %in% c("escaide2016")) %>%
  group_by(hashtag) %>%
  summarise(n=n()) %>% filter(!is.na(hashtag)) %>% arrange(-n) %>% top_n(n=40)

ggplot(df, aes(x=hashtag,y=n)) + geom_bar(stat="identity") +
  theme(axis.text.x=element_text(angle=90, hjust=1))
```

Distribution of the system used for the tweeting:

```{r}
ggplot( tweets, aes(source)) + geom_bar() + coord_flip()
```

Time series of the tweets (without interruptions):
```{r}
library(lubridate)

## Make POSIX function for dplyr re-use
make_posix <- . %>% mutate(time=lubridate::ymd_hms(paste0(year,"-",month,"-",day," ",hour,":00:00"),tz="CET"))

## All possible time points
ts <- tweets %>%
  mutate(hour = sprintf("%.02d",lubridate::hour(created_at)), day=lubridate::day(created_at),month=lubridate::month(created_at),year=lubridate::year(created_at)) %>%
  make_posix %>%
  group_by(time) %>% summarise(n=n())

allTimes <- expand.grid(year=2016, month=11, day=25:30, hour=sprintf("%.02d",0:23)) %>%
  make_posix %>%
  arrange(time) %>% mutate(n=0)

ts2 <- allTimes %>% left_join(ts, by="time") %>% group_by(time) %>%
  summarise(n=sum(n.x+n.y,na.rm=TRUE)) %>%
  filter(time > "2016-11-28 08:00:00 CET" & (time < "2016-11-30 12:00:00 CET"))
```


```{r}
p <- ggplot(ts2) + 
  geom_bar(aes(x=time, y=n), stat="identity",alpha=.8) + 
  theme(axis.text.x=element_text(angle=90, hjust=1)) + 
  ylab("No. of tweets") + 
  xlab("Time") + ggtitle(the_query) + 
  scale_x_datetime(date_minor_breaks="1 hour",date_breaks="1 hour",date_labels="Nov %d-%H:00") +
  theme(axis.text.x = element_text(angle = 60, size = 6))
p
```

Now we overlay the conference program

```{r}
conference_program_slots <- tibble::tribble(
  ~from, ~to, ~slot_type,
  "2016-11-28 09:00:00 CET",   "2016-11-28 10:30:00 CET", "Plenary Talks",
  "2016-11-28 11:00:00 CET",   "2016-11-28 12:40:00 CET", "Parallel Session",
  "2016-11-28 14:30:00 CET",   "2016-11-28 15:30:00 CET", "Parallel Session",
  "2016-11-28 15:30:00 CET",   "2016-11-28 16:30:00 CET", "Poster Session",
  "2016-11-28 17:00:00 CET",   "2016-11-28 18:30:00 CET", "Plenary Talks",

  "2016-11-29 09:00:00 CET",   "2016-11-29 10:30:00 CET", "Plenary Talks",
  "2016-11-29 11:00:00 CET",   "2016-11-29 12:40:00 CET", "Parallel Session",
  "2016-11-29 14:30:00 CET",   "2016-11-29 15:30:00 CET", "Parallel Session",
  "2016-11-29 15:30:00 CET",   "2016-11-29 16:30:00 CET", "Poster Session",
  "2016-11-28 17:00:00 CET",   "2016-11-28 18:30:00 CET", "Parallel Session",
  
  "2016-11-30 09:00:00 CET",   "2016-11-30 10:30:00 CET", "Plenary Talks",
  "2016-11-30 11:00:00 CET",   "2016-11-30 12:40:00 CET", "Parallel Session",
  "2016-11-30 14:30:00 CET",   "2016-11-30 15:30:00 CET", "Parallel Session",
  "2016-11-30 15:30:00 CET",   "2016-11-30 16:30:00 CET", "Poster Session",
  "2016-11-30 17:00:00 CET",   "2016-11-30 18:30:00 CET", "Plenary Talks"
) %>% mutate(from = as.POSIXct(from), to = as.POSIXct(to))
```

```{r}
p + geom_rect(data = filter(conference_program_slots, to < "2016-11-30 12:00:00 CET"), 
            aes(xmin = from, xmax = to, ymin = 0, ymax = max(ts$n), fill = slot_type), alpha = 0.4) + 
  theme(legend.position = "bottom") + 
  viridis::scale_fill_viridis(discrete = TRUE, name = "Session Type:")
```

## Outbreak detection

We shall use the EARS C method for performing outbreak detection.

```{r ears}
library(surveillance)
baseline <- 7
tweet_sts <- surveillance::sts(observed = ts2$n, # weekly number of cases
                               epoch = as.numeric(ts2$time))

monitored_tweets <- earsC(tweet_sts, control = list(baseline = baseline))
monitored_tweets_df <- as.data.frame(monitored_tweets)

monitored_tweets_df <- mutate(monitored_tweets_df, 
                          time = ts2$time[(baseline + 1):
                                           nrow(ts2) - 1])
ggplot(monitored_tweets_df) +
  geom_bar(aes(time, observed, fill = alarm), stat = "identity") +
  viridis::scale_fill_viridis(discrete = TRUE, name = "Alarm:") +
  geom_step(aes(time, upperbound)) + 
  theme(legend.position = "bottom") + 
  theme(axis.text.x=element_text(angle=90, hjust=1)) + 
  ylab("No. of tweets") + 
  xlab("Time") + ggtitle(the_query) + 
  scale_x_datetime(date_minor_breaks="1 hour",date_breaks="1 hour",date_labels="Nov %d-%H:00") +
  theme(axis.text.x = element_text(angle = 60, size = 6))

```

<center>
<blockquote class="twitter-tweet" data-lang="de"><p lang="en" dir="ltr">New blog entry: Please RT to help me evaluate my <a href="https://twitter.com/hashtag/zombie?src=hash">#zombie</a> monitoring system - <a href="https://t.co/b0gNfpJ0RM">https://t.co/b0gNfpJ0RM</a> <a href="https://twitter.com/hashtag/zombieattack?src=hash">#zombieattack</a> <a href="https://twitter.com/hashtag/biosurveillance?src=hash">#biosurveillance</a> <a href="https://twitter.com/hashtag/rstats?src=hash">#rstats</a> <a href="https://t.co/N3PTZBnaw4">pic.twitter.com/N3PTZBnaw4</a></p>&mdash; Michael HÃ¶hle (\@m_hoehle) <a href="https://twitter.com/m_hoehle/status/780037067183157248">25. September 2016</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
</center>

For a not so serious version of the above, see this
[blog post](http://staff.math.su.se/hoehle/blog/2016/09/25/sootb.html)
on how to detect zombie outbreaks using twitter.

